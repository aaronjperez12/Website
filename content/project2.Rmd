---
title: "Project 2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

##Introduction

The dataset I will be using in this project is called 'melanoma' and it contains the information for 205 people who were diagnosed with malignant melanoma and had their tumor removed by surgery. The data tells us the survival time in days since the operation ('time'), whether they died from melanoma, died from something else, or survived ('status'), the sex of the patient ('sex'), the age of the patient ('age'), the year the operation took place ('year'), the thickness of the tumor ('thickness'), and whether or not the tumor contained ulcers ('ulcer')

For 'status': 1=died from melanoma; 2=still alive; 3=died from some other cause

For 'sex': 1=male; 0=female

For 'ulcer': 1=present; 0=absent

```{r}
library(boot)
melanoma<-melanoma
```

##MANOVA

There are loads of assumptions in a MANOVA that are very hard to meet (multivariate normality of variables, homogeneity of within-group covariance matrices, no multicollinearity, etc.), so it is likely that we didn't meet all of them, but we will still proceed. First, I ran the MANOVA to test whether time, age, or thickness differ by patient status. The p-value obtained was <0.05, so we know that at least one of the groups differs.

```{r}
library(tidyverse)
man1 <- manova(cbind(time,age,thickness)~status, data=melanoma)
summary(man1)
```

<p>&nbsp;</p>

I then ran univariate ANOVAs for each variable, and found that time and thickness significantly differ by patient status, but age does not.
 
```{R}
summary.aov(man1)
```

<p>&nbsp;</p>

To test which specific statuses differ, I ran post hoc t-tests for these two variables. I found that the time a patient stays alive post-surgery is significantly different between groups 1 and 2, and groups 2 and 3, but not groups 1 and 3. Thickness of the tumor only differed between groups 1 and 2.

```{R}
melanoma %>% group_by(status) %>% summarize(mean(time),mean(age),mean(thickness))

pairwise.t.test(melanoma$time,melanoma$status, p.adj="none")
pairwise.t.test(melanoma$thickness,melanoma$status, p.adj="none")
```

<p>&nbsp;</p>

All together I ran 10 different tests to get to this conclusion, which means that my probability of at least one type I error is about 40%. To adjust for this, I would use an alpha of 0.005 instead of 0.05. Using this new alpha, all of my significant test results still remain significant.

```{R}
1-0.95^10

0.05/10
```

##Randomization Test

I ran a randomization test to see whether there was a difference in tumor thickness between males and females.

*Null hypothesis: Mean tumor thickness is the same for males and females*
*Alternative Hypothesis: Mean tumor thickness is not the same for males and females*

```{r}
male <- melanoma %>% filter(sex==1) %>% select(thickness) %>% unlist(melanoma[1,])
female <- melanoma %>% filter(sex==0) %>% select(thickness) %>% unlist(melanoma[1,])

mel<-data.frame(sex=c(rep("male",79),rep("female",126)),thickness=c(male,female))

ggplot(mel,aes(thickness,fill=sex))+geom_histogram(bins=6.5)+facet_wrap(~sex,ncol=2)+theme(legend.position="none")

mel%>%group_by(sex)%>%
  summarize(means=mean(thickness))%>%summarize(`mean_diff:`=diff(means))

rand_dist<-vector()

for(i in 1:5000){
new<-data.frame(thickness=sample(mel$thickness),sex=mel$sex) 
rand_dist[i]<-mean(new[new$sex=="male",]$thickness)-
              mean(new[new$sex=="female",]$thickness)}

{hist(rand_dist,main="",ylab=""); abline(v = 1.124711,col="red")}

mean(rand_dist>1.124711)*2
```

A p-value < 0.05 tells us that we can reject the null hypothesis. The mean tumor thickness significantly differs between males and females.

##Linear Regression

This linear regression model will be predicting the length of time a patient survives after surgery, from their age and the thickness of their tumor

```{r}
melanoma$thickness_c <- melanoma$thickness - mean(melanoma$thickness)
melanoma$age_c <- melanoma$age - mean(melanoma$age)
fit_reg<-lm(time ~ thickness_c*age_c, data=melanoma)
```

<p>&nbsp;</p>

As we can see from these coefficients, a person of average tumor thickness and average age is expected to live for about 2172 days after surgery. When controlling for thickness, the amount of time a patient lives after surgery decreases by 17.5 days for each year that they are above the average age. When controlling for age, the amount of time a patient lives after surgery decreases by 55.8 days for every mm that their tumor is above the average. The interaction term tells us that thickness has a slightly less effect on time as age increases.

```{R}
summary(fit_reg)
```

<p>&nbsp;</p>

When plotting the linear regression model, we can see that both age and thickness have a linear relationship with time. The assumption of linearity is met.

```{R}
melanoma%>%ggplot(aes(age_c,time))+geom_point()+geom_smooth(method = 'lm',se=F)
```

```{r}
melanoma%>%ggplot(aes(thickness_c,time))+geom_point()+geom_smooth(method = 'lm',se=F)
```

<p>&nbsp;</p>

To test the assumption of homoskedasticity, I ran a Breuch-Pagan test on the linear regression model. I obtained a p-value < 0.05, which means that we can reject the null hypothesis that our model is homoskedastic. This is also clearly illustrated by the plot below. The assumption of homoskedasticity is not met.

```{R}
library(sandwich)
library(lmtest)
resids<-fit_reg$residuals
fitvals<-fit_reg$fitted.values
bptest(fit_reg)
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
```

<p>&nbsp;</p>

To test the assumption of normality, I ran both a Kolmogorov-Smirnov (KS) test and a Shapiro-Wilk test. Both of these tests came out with p-values > 0.05 which means that we fail to reject the null hypothesis of normality. This is also illustrated in the plots below. The assumption of normality is met.*

```{R}
ks.test(resids, "pnorm", mean=0, sd(resids))
shapiro.test(resids) 

ggplot()+geom_histogram(aes(resids), bins=20)
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids))
```

<p>&nbsp;</p>

When recomputing the regression with robust standard errors, thickness is no longer a significant predictor of time. The standard error of thickness in the model using the robust standard error was much greater than that of the original regression. However, age is still a significant predictor of time.

```{R}
coeftest(fit_reg, vcov = vcovHC(fit_reg))
```

Using the adjusted R-squared value, we know that this model explains about 11.5% of the variation.

<p>&nbsp;</p>

Upon running the regression model with bootstrapped standard errors, we can see that the bootstrapped standard errors and the robust standard errors were very similar, while the original standard errors were much lower. This makes sense because bootstrapping removes the need for any assumptions, since we are basically taking random samples from the dataset, and robust standard errors penalize you for the addition of statistical noise. We would expect both of these to be more accurate and use a greater standard error than the original regression.

```{r}
samp_distn<-replicate(5000, {
  boot_dat<-melanoma[sample(nrow(melanoma),replace=TRUE),]
  fit_boot<-lm(time~thickness_c*age_c,data=boot_dat) 
  coef(fit_boot)
})

samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)
```

##Logistic Regression

I ran a logistic regression predicting whether or not a patient had ulcers on their tumor, from thickness of the tumor and post-surgery survival time.

It turns out that both thickness and time are significant measures in determining whether a patient had an ulcer or not. For a patient with average thickness and average survival time, the odds of them having an ulcer is 0.86. For every 1 mm increase in thickness, the odds of having an ulcer increase by a factor of 1.54 when controlling for time. For each extra day of survival above the average, the odds of of having an ulcer decrease by a factor of 0.99 when controlling for thickness.

```{r}
melanoma$time_c <- melanoma$time - mean(melanoma$time)
fit_log<-glm(ulcer~thickness_c+time_c,data=melanoma,family="binomial")
coeftest(fit_log)
exp(coef(fit_log))
```



```{R}
melanoma$prob<-predict(fit_log,type="response") 
melanoma$predicted<-ifelse(melanoma$prob>.5,"1","0") 

melanoma$logit<-predict(fit_log) 
melanoma$outcome<-factor(melanoma$ulcer,levels=c("1","0"))

table(truth=melanoma$outcome, prediction=melanoma$predicted)%>%addmargins
```

Accuracy:

```{R}
(100+53)/205 
```

Sensitivity (TPR):

```{r}
53/90 
```

Specificity (TNR):

```{r}
100/115 
```

Recall (PPV):

```{r}
53/68 
```

Although the model has relatively good accuracy, specificity, and recall, the sensitivity of our model is not very good at all.

```{R}
ggplot(melanoma,aes(logit, fill=outcome))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)
```

<p>&nbsp;</p>

A way that we can interpret the AUC is that 81.7% of the time, a randomly selected patient who had ulcers will have a higher predicted probability from the model than a randomly selected patient who didn't have ulcers.

```{R}
library(plotROC)
ROCplot<-ggplot(melanoma)+geom_roc(aes(d=ulcer,m=prob), n.cuts=0)
ROCplot

calc_auc(ROCplot)
```

<p>&nbsp;</p>

When performing a 10-fold cross validation on the model, we obtain an AUC that is very similar to the original AUC, which means that our model is not overfitting very much at all. The values for accuracy, sensitivity, and recall were very similar as well.

```{R}
## GIVE IT PREDICTED PROBS AND TRUTH LABELS, RETURNS VARIOUS DIAGNOSTICS
class_diag <- function(probs,truth){
#CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
}

set.seed(1234)
k=10 #choose number of folds
data<-melanoma[sample(nrow(melanoma)),] #randomly order rows
folds<-cut(seq(1:nrow(melanoma)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
## Create training and test sets
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$ulcer ## Truth labels for fold i
## Train model on training set (all but fold i)
fitfin<-glm(ulcer~thickness+time,data=train,family="binomial")
## Test model on test set (fold i)
probs<-predict(fitfin,newdata = test,type="response")
## Get diagnostics for fold i
diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean) 
```

##LASSO Regression

*When running a LASSO regression, we see that the variables that sufficiently predict whether or not a patient had ulcers are time, status, and thickness. I then performed a 10-fold cross validation using this model. It turns out that this model actually gives us a higher accuracy than before. This makes sense because the LASSO regression tries to get rid of noisy variables so that we only include those that will be good predictors.

```{r}
melanoma$outcome <- NULL

library(glmnet)
y<-as.matrix(melanoma$ulcer)
x<-melanoma%>%dplyr::select(-ulcer,-thickness_c,-age_c,-prob,-predicted,-logit,-time_c)%>%mutate_all(scale)%>%as.matrix

cv<-cv.glmnet(x,y) #this picks an optimal value for lambda (smallest MSE) via 10-fold CV

lasso1<-glmnet(x,y,lambda=cv$lambda.1se)
coef(lasso1)

set.seed(1234)
k=10 #choose number of folds

data1<-melanoma[sample(nrow(melanoma)),] #randomly order rows
folds<-cut(seq(1:nrow(melanoma)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data1[folds!=i,] 
  test<-data1[folds==i,]
  truth<-test$ulcer
  
  ## Train model on training set
  fit2<-glm(ulcer~time+status+thickness,data=train,family="binomial")
  probs<-predict(fit2,newdata = test,type="response")
  
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth))
}

diags%>%summarize_all(mean)
```

